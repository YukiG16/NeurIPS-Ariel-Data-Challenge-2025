{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":101849,"databundleVersionId":12846694,"sourceType":"competition"},{"sourceId":12303949,"sourceType":"datasetVersion","datasetId":7755350},{"sourceId":12309150,"sourceType":"datasetVersion","datasetId":7758622},{"sourceId":12388933,"sourceType":"datasetVersion","datasetId":7812079}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport scipy.stats\nimport pywt\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm\n\nclass Config:\n    DATA_PATH = '/kaggle/input/ariel-data-challenge-2025/'\n    PREPROCESSED_PATH = '/kaggle/input/ariel-data-challenge-2025-af-npy/'\n    OUTPUT_PATH = '/kaggle/working/'\n    TRAIN_LABELS_PATH = os.path.join(DATA_PATH, 'train.csv')\n    TRAIN_STAR_INFO_PATH = os.path.join(DATA_PATH, 'train_star_info.csv')\n    TEST_STAR_INFO_PATH = os.path.join(DATA_PATH, 'test_star_info.csv')\n    SAMPLE_SUBMISSION_PATH = os.path.join(DATA_PATH, 'sample_submission.csv')\n    WAVELENGTHS_PATH = os.path.join(DATA_PATH, 'wavelengths.csv')\n    VALIDATION_FOLDS = 5\n    RANDOM_STATE = 42\n    QUANTILES = [0.05, 0.50, 0.95]\n    XGB_PARAMS = {\n        'n_estimators': 400,\n        'learning_rate': 0.04,\n        'max_depth': 6,\n        'subsample': 0.8,\n        'colsample_bytree': 0.7,\n        'random_state': RANDOM_STATE,\n        'tree_method': 'hist',\n        'verbosity': 0\n    }\n    LGB_PARAMS = {\n        'learning_rate': 0.03,\n        'num_leaves': 64,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 1,\n        'seed': RANDOM_STATE,\n        'n_estimators': 1000,\n        'verbosity': -1\n    }\n\nconfig = Config()\n\ndef f_read_and_preprocess(dataset, planet_ids):\n    f_raw = np.full((len(planet_ids), 67500), np.nan, dtype=np.float32)\n    print(f\"Processing FGS1 data for {len(planet_ids)} planets...\")\n    for i, pid in enumerate(tqdm(planet_ids, desc=\"Loading FGS1 signals\")):\n        df = pl.read_parquet(f\"{config.DATA_PATH}/{dataset}/{int(pid)}/FGS1_signal_0.parquet\")\n        mean_signal = df.cast(pl.Int32).sum_horizontal().to_numpy()/1024\n        f_raw[i] = mean_signal[1::2] - mean_signal[0::2]\n    return f_raw\n\n\ndef a_read_and_preprocess(dataset, planet_ids):\n    a_raw = np.full((len(planet_ids), 5625), np.nan, dtype=np.float32)\n    print(f\"Processing AIRS data for {len(planet_ids)} planets...\")\n    for i, pid in enumerate(tqdm(planet_ids, desc=\"Loading AIRS signals\")):\n        df = pl.read_parquet(f\"{config.DATA_PATH}/{dataset}/{int(pid)}/AIRS-CH0_signal_0.parquet\")\n        mean_signal = df.cast(pl.Int32).sum_horizontal().to_numpy()/(32*356)\n        a_raw[i] = mean_signal[1::2] - mean_signal[0::2]\n    return a_raw\n\n\ndef make_features(f_raw, a_raw, star_info):\n    print(\"Generating features...\")\n    \n    # FGS1 features \n    fgs_pre = f_raw[:, :20500]; fgs_post = f_raw[:, 47000:]\n    fgs_unmean = (fgs_pre.mean(1)+fgs_post.mean(1))/2\n    fgs_unstd = (fgs_pre.std(1)+fgs_post.std(1))/2\n    transit = f_raw[:,23500:44000]\n    feats = {}\n    \n    # FGS1 slice features\n    for i in tqdm(range(5), desc=\"FGS1 slice features\", leave=False):\n        m = transit[:,i*4100:(i+1)*4100].mean(1)\n        feats[f'fgs_slice_{i+1}'] = (fgs_unmean - m)/fgs_unmean\n    \n    feats['fgs_transit_std'] = transit.std(1)\n    feats['fgs_transit_skew'] = scipy.stats.skew(transit, axis=1)\n    \n    # FFT features\n    fft = np.fft.fft(transit, axis=1)\n    for i in tqdm(range(1,6), desc=\"FGS1 FFT features\", leave=False): \n        feats[f'fgs_fft_{i}'] = np.abs(fft[:,i])\n    \n    # Wavelet features\n    for lvl in tqdm(range(1,4), desc=\"FGS1 wavelet features\", leave=False):\n        c = pywt.wavedec(transit, 'db4', level=lvl, axis=1)[0]\n        feats[f'fgs_wav_std_{lvl}'] = np.std(c,1)\n\n    # AIRS features: central window\n    a_tr = a_raw[:,500:4500]\n    feats['airs_std'] = a_tr.std(1)\n    feats['airs_skew'] = scipy.stats.skew(a_tr,axis=1)\n    \n    # AIRS FFT features\n    fft_a = np.fft.fft(a_tr, axis=1)\n    for i in tqdm(range(1,6), desc=\"AIRS FFT features\", leave=False): \n        feats[f'airs_fft_{i}'] = np.abs(fft_a[:,i])\n    \n    # AIRS wavelet features\n    for lvl in tqdm(range(1,4), desc=\"AIRS wavelet features\", leave=False):\n        c = pywt.wavedec(a_tr, 'db4', level=lvl, axis=1)[0]\n        feats[f'airs_wav_std_{lvl}'] = np.std(c,1)\n\n    # Metadata & interactions\n    meta = star_info.fillna(star_info.median())\n    meta['Mp_P'] = meta['Mp']/ (meta['P']+1e-6)\n    meta['logRs_logTs'] = np.log1p(meta['Rs'])*np.log1p(meta['Ts'])\n\n    X = pd.DataFrame(feats, index=star_info.index)\n    X = pd.concat([X, meta], axis=1)\n    X.fillna(0, inplace=True)\n    print(f\"Generated {X.shape[1]} features for {X.shape[0]} samples\")\n    return X\n\ndef official_score(y_true, y_pred, sigma, mu0, sigma0):\n    y_true,y_pred,sigma = map(np.array, (y_true,y_pred,sigma))\n    sigma = np.clip(sigma,1e-15,None)\n    base = scipy.stats.norm.logpdf(y_true,loc=y_true,scale=sigma0)\n    glp = scipy.stats.norm.logpdf(y_true,loc=y_pred,scale=sigma)\n    gm = scipy.stats.norm.logpdf(y_true,loc=mu0,scale=sigma0)\n    return ((glp-gm)/(base-gm+1e-9)).mean()\n\ndef train_and_calibrate():\n    print(\"Loading training data...\")\n    labels = pd.read_csv(config.TRAIN_LABELS_PATH, index_col='planet_id')\n    stars = pd.read_csv(config.TRAIN_STAR_INFO_PATH, index_col='planet_id').loc[labels.index]\n    \n    print(\"Loading preprocessed arrays...\")\n    f_raw = np.load(config.PREPROCESSED_PATH+'f_raw_train.npy')\n    a_raw = np.load(config.PREPROCESSED_PATH+'a_raw_train.npy')\n    \n    X = make_features(f_raw, a_raw, stars)\n    y = labels.values\n    mu0, sigma0 = y.mean(), y.std()\n\n    # OOF containers\n    oof_med = np.zeros_like(y)\n    oof_sig = np.zeros_like(y)\n    raw_sig_vals = []\n\n    print(f\"Starting {config.VALIDATION_FOLDS}-fold cross-validation...\")\n    kf = KFold(n_splits=config.VALIDATION_FOLDS, shuffle=True, random_state=config.RANDOM_STATE)\n    \n    for fold, (tr, val) in enumerate(tqdm(kf.split(X), total=config.VALIDATION_FOLDS, desc=\"CV Folds\")):\n        print(f\"\\n--- Fold {fold + 1}/{config.VALIDATION_FOLDS} ---\")\n        X_tr, X_val = X.iloc[tr], X.iloc[val]\n        y_tr, y_val = y[tr], y[val]\n        \n        # train median with stacking: XGB + LGB + Ridge\n        preds = []\n        models = ['XGBoost', 'LightGBM']\n        \n        for idx, mdl in enumerate(tqdm(['xgb','lgb'], desc=\"Training models\", leave=False)):\n            if mdl=='xgb':\n                print(f\"  Training {models[idx]}...\")\n                m = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:squarederror')\n                m.fit(X_tr, y_tr)\n            else:\n                print(f\"  Training {models[idx]}...\")\n                m = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='regression'))\n                m.fit(X_tr, y_tr)\n            preds.append(m.predict(X_val))\n        \n        # stack\n        print(\"  Stacking predictions...\")\n        stack_tr = np.stack(preds,2).mean(2)\n        ridge = Ridge().fit(stack_tr, y_val)\n        med = ridge.predict(stack_tr)\n        oof_med[val] = med\n        \n        # quantile raw\n        print(\"  Training quantile regressors...\")\n        lower = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.05)).fit(X_tr,y_tr).predict(X_val)\n        upper = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.95)).fit(X_tr,y_tr).predict(X_val)\n        raw = (upper-lower)/3.29\n        oof_sig[val] = raw\n        raw_sig_vals.append((raw.flatten(), np.abs(y_val-med).flatten()))\n\n    # isotonic calibration\n    print(\"\\nPerforming isotonic calibration...\")\n    raw_all = np.concatenate([r for r,_ in raw_sig_vals])\n    true_err = np.concatenate([e for _,e in raw_sig_vals])\n    ir = IsotonicRegression(out_of_bounds='clip').fit(raw_all, true_err)\n    \n    # save calibration\n    print(\"Saving calibrator and features...\")\n    with open(config.OUTPUT_PATH+'calibrator.pkl','wb') as f: pickle.dump(ir,f)\n    # save features\n    with open(config.OUTPUT_PATH+'features.pkl','wb') as f: pickle.dump(X.columns.tolist(),f)\n    \n    # return OOF score\n    sig_cal = ir.predict(oof_sig.flatten()).reshape(oof_sig.shape)\n    print(f\"\\nOOF official score: {official_score(y,oof_med,sig_cal,mu0,sigma0):.6f}\")\n\ndef make_submission():\n    print(\"Creating submission...\")\n    # load\n    print(\"Loading submission data...\")\n    sample = pd.read_csv(config.SAMPLE_SUBMISSION_PATH, index_col='planet_id')\n    stars = pd.read_csv(config.TEST_STAR_INFO_PATH, index_col='planet_id')\n    waves = pd.read_csv(config.WAVELENGTHS_PATH)\n    \n    print(\"Loading saved models...\")\n    cols = pickle.load(open(config.OUTPUT_PATH+'features.pkl','rb'))\n    ir = pickle.load(open(config.OUTPUT_PATH+'calibrator.pkl','rb'))\n\n    # data\n    f_raw = f_read_and_preprocess('test', stars.index)\n    a_raw = a_read_and_preprocess('test', stars.index)\n    X_test = make_features(f_raw,a_raw,stars)[cols]\n\n    # median ensemble\n    print(\"Training final models on full data...\")\n    print(\"  Training XGBoost...\")\n    pred_xgb = xgb.XGBRegressor(**config.XGB_PARAMS).fit(X_test, np.zeros((0, waves.shape[1]))).predict(X_test)\n    print(\"  Training LightGBM...\")\n    pred_lgb = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='regression')).fit(X_test, np.zeros((0,waves.shape[1]))).predict(X_test)\n    print(\"  Stacking predictions...\")\n    med = Ridge().fit(np.stack([pred_xgb,pred_lgb],2).mean(2), np.zeros((0,waves.shape[1]))).predict(np.stack([pred_xgb,pred_lgb],2).mean(2))\n    \n    # quantiles\n    print(\"  Training quantile regressors...\")\n    lower = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.05)).fit(X_test,np.zeros((0,waves.shape[1]))).predict(X_test)\n    upper = MultiOutputRegressor(lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.95)).fit(X_test,np.zeros((0,waves.shape[1]))).predict(X_test)\n    raw = (upper-lower)/3.29\n    \n    print(\"  Applying calibration...\")\n    sig = ir.predict(raw.flatten()).reshape(raw.shape)\n\n    # build df\n    print(\"Building submission DataFrame...\")\n    df_med = pd.DataFrame(np.clip(med,0,None), index=sample.index, columns=waves.columns)\n    df_sig = pd.DataFrame(sig, index=sample.index, columns=[f'sigma_{i+1}' for i in range(waves.shape[1])])\n    sub = pd.concat([df_med, df_sig], axis=1)\n    \n    print(\"Saving submission...\")\n    sub.to_csv('submission.csv')\n    print('✓ submission.csv created successfully!')\n\n# ==============================================================================\n# ENTRY POINT\n# ==============================================================================\n#if __name__=='__main__':\n#    mode = os.getenv('MODE','train')  # set MODE=train or MODE=submit\n#    print(f\"Running in {mode} mode...\")\n#    if mode=='train': \n#        train_and_calibrate()\n#    else: \n#        make_submission()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T00:26:16.753275Z","iopub.execute_input":"2025-07-07T00:26:16.753533Z","iopub.status.idle":"2025-07-07T00:26:22.777240Z","shell.execute_reply.started":"2025-07-07T00:26:16.753512Z","shell.execute_reply":"2025-07-07T00:26:22.776663Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def make_submission():\n    print(\"Creating submission...\")\n    \n    labels = pd.read_csv(config.TRAIN_LABELS_PATH, index_col='planet_id')\n    stars_train = pd.read_csv(config.TRAIN_STAR_INFO_PATH, index_col='planet_id').loc[labels.index]\n    f_train = np.load(config.PREPROCESSED_PATH+'f_raw_train.npy')\n    a_train = np.load(config.PREPROCESSED_PATH+'a_raw_train.npy')\n    X_train = make_features(f_train, a_train, stars_train)\n    y_train = labels.values\n    print(f\"Final training on {X_train.shape[0]} samples x {X_train.shape[1]} features.\")\n\n    sample = pd.read_csv(config.SAMPLE_SUBMISSION_PATH, index_col='planet_id')\n    stars_test = pd.read_csv(config.TEST_STAR_INFO_PATH, index_col='planet_id')\n    waves = pd.read_csv(config.WAVELENGTHS_PATH)\n\n    f_test = f_read_and_preprocess('test', stars_test.index)\n    a_test = a_read_and_preprocess('test', stars_test.index)\n    X_test = make_features(f_test, a_test, stars_test)\n\n    print(\"  Training XGBoost on full data...\")\n    xgb_full = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:squarederror')\n    xgb_full.fit(X_train, y_train)\n\n    print(\"  Training LightGBM on full data...\")\n    lgb_full = MultiOutputRegressor(\n        lgb.LGBMRegressor(**config.LGB_PARAMS, objective='regression')\n    )\n    lgb_full.fit(X_train, y_train)\n\n    print(\"  Forming median ensemble...\")\n    pred_x = xgb_full.predict(X_test)\n    pred_l = lgb_full.predict(X_test)\n    med = Ridge().fit(\n        np.stack([xgb_full.predict(X_train), lgb_full.predict(X_train)], axis=2).mean(2),\n        y_train\n    ).predict(np.stack([pred_x, pred_l], axis=2).mean(2))\n\n    print(\"  Training quantile regressors...\")\n    lower_mdl = MultiOutputRegressor(\n        lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.05)\n    ).fit(X_train, y_train)\n    upper_mdl = MultiOutputRegressor(\n        lgb.LGBMRegressor(**config.LGB_PARAMS, objective='quantile', alpha=0.95)\n    ).fit(X_train, y_train)\n    lower = lower_mdl.predict(X_test)\n    upper = upper_mdl.predict(X_test)\n    raw_sigma = (upper - lower) / 3.29\n\n    print(raw_sigma)\n\n    #ir = pickle.load(open(\"/kaggle/input/neurips-pkls/calibrator.pkl\",'rb'))\n    sigma = raw_sigma.flatten().reshape(raw_sigma.shape)\n\n    df_med = pd.DataFrame(np.clip(med, 0, None),\n                          index=sample.index, columns=waves.columns)\n    df_sig = pd.DataFrame(sigma,\n                          index=sample.index,\n                          columns=[f'sigma_{i+1}' for i in range(waves.shape[1])])\n    #df_sig = pd.DataFrame(raw_sigma,\n                          #index=sample.index,\n                          #columns=[f'sigma_{i+1}' for i in range(waves.shape[1])])\n    submission = pd.concat([df_med, df_sig], axis=1)\n    submission.to_csv('submission.csv')\n    print(\"submission created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T00:26:22.778373Z","iopub.execute_input":"2025-07-07T00:26:22.778943Z","iopub.status.idle":"2025-07-07T00:26:22.788078Z","shell.execute_reply.started":"2025-07-07T00:26:22.778922Z","shell.execute_reply":"2025-07-07T00:26:22.787516Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"make_submission()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T00:26:22.788850Z","iopub.execute_input":"2025-07-07T00:26:22.789499Z","iopub.status.idle":"2025-07-07T00:46:00.923051Z","shell.execute_reply.started":"2025-07-07T00:26:22.789474Z","shell.execute_reply":"2025-07-07T00:46:00.922167Z"}},"outputs":[{"name":"stdout","text":"Creating submission...\nGenerating features...\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Generated 35 features for 1100 samples\nFinal training on 1100 samples x 35 features.\nProcessing FGS1 data for 1 planets...\n","output_type":"stream"},{"name":"stderr","text":"Loading FGS1 signals: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Processing AIRS data for 1 planets...\n","output_type":"stream"},{"name":"stderr","text":"Loading AIRS signals: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating features...\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Generated 35 features for 1 samples\n  Training XGBoost on full data...\n  Training LightGBM on full data...\n  Forming median ensemble...\n  Training quantile regressors...\n[[ 1.62281851e-05  5.45407689e-04  6.15054629e-05  4.09784934e-04\n   2.19479510e-04  4.36732769e-04  4.60180664e-05  4.59631481e-04\n   3.84757662e-04  5.55488075e-04  3.46087838e-04  2.18335866e-04\n   6.15342206e-04  3.22037099e-04  2.73908537e-04  4.41517333e-04\n   3.73951051e-04  6.22037420e-04  6.05126574e-04  4.39042981e-04\n   4.12717429e-04  5.29294003e-04  3.65585705e-04  5.49047041e-04\n   5.81348449e-04  5.44117142e-04  3.76045376e-04  5.54531321e-04\n   7.48097231e-04  4.55351715e-04  3.58565403e-04  7.12598537e-04\n   4.97126259e-04  4.19664699e-04  3.76780441e-04  3.44794052e-04\n   2.55964322e-04  3.31586362e-04  5.95279003e-04  5.27430259e-04\n   3.56778800e-04  3.40738771e-04  3.82365743e-04  4.75274765e-04\n   3.76874316e-04  5.18045579e-04  3.30334786e-04  5.33421467e-04\n   5.27068551e-04  4.66945626e-04  6.90194486e-04  5.93712014e-04\n   4.96046393e-04  4.84651243e-04  5.02812448e-04  3.93470363e-04\n   3.78634603e-04  3.46663984e-04  4.05674727e-04  2.43023775e-04\n   3.15517610e-04  1.22875255e-05  1.85573044e-04  3.27332721e-04\n   2.72147614e-04  4.39326984e-04  3.46960326e-04  3.53993616e-04\n   3.49587799e-04  4.09268727e-04  8.70494129e-05  1.33500695e-04\n   1.62081390e-04  4.83753162e-04  3.64202687e-04  2.84375743e-04\n   2.95326853e-04  2.13381450e-04  3.12874096e-04  2.87778389e-04\n   2.55650888e-04 -4.39981062e-06  1.35362729e-04  1.61436325e-04\n   3.75852260e-04  2.46012638e-04  5.71668832e-05  2.48356628e-04\n   2.66776949e-04  1.96299461e-04  1.84637115e-04  4.05204968e-04\n   2.85596667e-04  2.22848804e-04 -3.53421897e-05  2.67373186e-04\n   1.40308244e-04  1.32926358e-04  1.90867392e-04  1.25869709e-04\n   5.01917271e-04  6.69823115e-05  6.40386222e-04  1.66612851e-04\n   3.57913870e-04  2.71938687e-04  1.69404659e-04  9.97180154e-05\n   3.63190456e-04  2.54613546e-04  3.43908080e-04  5.16759829e-04\n   3.22713492e-04  4.31578574e-04  3.56032917e-04  1.52706281e-04\n   4.63109128e-04  1.28588857e-04  2.80092170e-04  2.70128498e-04\n   5.26424558e-04  3.19353358e-04  2.47876321e-04  3.02317332e-05\n   7.41796845e-05  2.95506943e-04  2.97694646e-04  1.02289442e-04\n   2.92743174e-04 -3.32482102e-05  3.31483729e-04  1.69646853e-04\n   1.78424715e-04  3.29464732e-04  3.89702049e-04  2.18895399e-04\n   3.55497316e-04  2.05755759e-04  1.73390157e-04  3.46383174e-04\n   1.16399839e-04  8.00497601e-05  1.42396304e-04  3.37484653e-04\n   2.50868006e-04  3.12179221e-04  3.55217195e-04  2.73628697e-04\n   3.86167994e-04  2.26890378e-04  1.97283101e-04  2.12687764e-04\n   4.14789999e-04  2.83467732e-04  1.75203661e-04  3.59227485e-04\n   4.65384015e-04  2.73855560e-04  9.02884499e-05  4.33799748e-04\n   3.27974669e-04  1.63820569e-04  4.38599785e-04  3.86701520e-04\n   3.32610279e-04  2.77499135e-04  1.06179372e-04  3.41193950e-04\n   4.12676013e-04  4.43881564e-04  4.08001924e-04  2.39570621e-04\n   5.69635659e-04  3.56853166e-04  3.77939195e-04  2.22806433e-04\n   2.86287844e-04  4.40695273e-04  2.19122634e-04  2.94852439e-04\n   2.69289348e-04  1.27202251e-04  3.51381391e-04  2.70861151e-04\n   4.45045238e-04  3.80377900e-04  2.99174097e-04 -2.86655143e-05\n   4.33344074e-04  2.69890341e-04  5.11964222e-04  5.65176344e-04\n   5.83276847e-04  1.21830769e-04  1.25130532e-04  3.17578305e-04\n   1.43423752e-04  5.92432142e-04  5.54832382e-04  2.73041854e-04\n   3.24601692e-04  4.34821112e-04  2.55190215e-04  1.44726185e-04\n   5.62766484e-04  4.31684417e-04  1.43492881e-04  3.30695270e-04\n   2.15642705e-04  1.63044875e-04  3.10294647e-04  4.43417650e-04\n   6.58270999e-04  3.80544395e-04  5.30442412e-04  3.90999660e-04\n   2.32585984e-04  2.80001239e-04  3.56223911e-04  4.00745738e-04\n   4.10628947e-04  3.70917859e-04  4.43265662e-04  2.43228634e-04\n   6.81815493e-04  2.63730814e-04  4.33684987e-04  5.80600905e-04\n   6.75287925e-04  5.51512733e-04  5.92620897e-04  4.83276055e-04\n   4.01808871e-04  2.82406354e-04  5.85158278e-04  1.55028898e-04\n   3.99482716e-04  4.42095401e-04  5.57633237e-04  3.15600830e-04\n   4.14621088e-04  4.34542378e-04  2.98605789e-04  6.12928555e-04\n   3.51916409e-04  1.22662216e-04  1.64185372e-04  1.88244758e-04\n   4.54909721e-04  1.68423393e-04  4.23289705e-04  4.17463274e-04\n   5.35451211e-04  2.31570680e-04  7.65985561e-04  1.76861743e-04\n   4.59511896e-04  2.87547495e-04  5.50958023e-04  3.90171328e-04\n   3.03511299e-04  1.76503573e-04  3.14696888e-04  8.55779616e-04\n   6.98624537e-04  4.76455523e-04  3.82434554e-04  3.79071450e-04\n   2.12400712e-04  5.22009136e-04  2.81013766e-04  1.96594793e-04\n   3.45283889e-04  3.33863303e-04  3.18955740e-04  2.31976348e-04\n   5.16467587e-04  8.32502403e-05  5.28084120e-04  3.95565939e-04\n   3.54385446e-04  3.99978021e-04  3.56585173e-04]]\nsubmission created successfully!\n","output_type":"stream"}],"execution_count":3}]}