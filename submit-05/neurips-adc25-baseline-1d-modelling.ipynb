{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":101849,"databundleVersionId":12846694,"sourceType":"competition"},{"sourceId":9774816,"sourceType":"datasetVersion","datasetId":5548325},{"sourceId":192766898,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Orginal Notobeok: \n\nhttps://www.kaggle.com/code/ilu000/ariel25-baseline-submission-1d-modelling","metadata":{}},{"cell_type":"code","source":"!pip install scikit_learn==1.5.1 --no-index --find-links=/kaggle/input/ariel24-pip-installs","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:00.991457Z","iopub.execute_input":"2025-07-10T01:08:00.992082Z","iopub.status.idle":"2025-07-10T01:08:03.604150Z","shell.execute_reply.started":"2025-07-10T01:08:00.992037Z","shell.execute_reply":"2025-07-10T01:08:03.603167Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/ariel24-pip-installs\n\u001b[33mWARNING: Location '/kaggle/input/ariel24-pip-installs' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scikit_learn==1.5.1 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for scikit_learn==1.5.1\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# Set to \"1\" to directly force test df, this is much quicker to commit\nos.environ[\"KAGGLE_IS_COMPETITION_RERUN\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:03.606975Z","iopub.execute_input":"2025-07-10T01:08:03.607817Z","iopub.status.idle":"2025-07-10T01:08:03.612564Z","shell.execute_reply.started":"2025-07-10T01:08:03.607780Z","shell.execute_reply":"2025-07-10T01:08:03.611761Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%writefile preprocess.py\n\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom tqdm import tqdm\nimport multiprocessing as mp\nfrom astropy.stats import sigma_clip\nimport os\nimport torch\nimport torch.nn.functional as F\n\n\nROOT = \"/kaggle/input/ariel-data-challenge-2025/\"\nVERSION = \"v2\"\n\nBINNING = 15\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    MODE = \"test\"\nelse:\n    MODE = \"train\"\n\n\nsensor_sizes_dict = {\n    \"AIRS-CH0\": [[11250, 32, 356], [32, 356]],\n    \"FGS1\": [[135000, 32, 32], [32, 32]],\n}  # input, mask\n\n# 16 center pixels, rest is just noise\ncl = 8\ncr = 24\n\n\ndef get_gain_offset():\n    \"\"\"\n    Get the gain and offset for a given planet and sensor\n\n    Unlike last year's challenge, all planets use the same adc_info.\n    We can just hard code it.\n    \"\"\"\n    gain = 0.4369\n    offset = -1000.0\n    return gain, offset\n\n\ndef read_data(planet_id, sensor, mode):\n    \"\"\"\n    Read the data for a given planet and sensor\n    \"\"\"\n    # get all noise correction frames and signal\n    signal = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_signal_0.parquet\",\n        engine=\"pyarrow\",\n    )\n    dark_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/dark.parquet\",\n        engine=\"pyarrow\",\n    )\n    dead_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/dead.parquet\",\n        engine=\"pyarrow\",\n    )\n    linear_corr_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/linear_corr.parquet\",\n        engine=\"pyarrow\",\n    )\n    flat_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/flat.parquet\",\n        engine=\"pyarrow\",\n    )\n    # read_frame = pd.read_parquet(\n    #     f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration/read.parquet\",\n    #     engine=\"pyarrow\",\n    # )\n\n    # reshape to sensor shape and cast to float64\n    signal = signal.values.astype(np.float64).reshape(sensor_sizes_dict[sensor][0])[\n        :, cl:cr, :\n    ]\n    dark_frame = dark_frame.values.astype(np.float64).reshape(\n        sensor_sizes_dict[sensor][1]\n    )[cl:cr, :]\n    dead_frame = dead_frame.values.reshape(sensor_sizes_dict[sensor][1])[cl:cr, :]\n    flat_frame = flat_frame.values.astype(np.float64).reshape(\n        sensor_sizes_dict[sensor][1]\n    )[cl:cr, :]\n    # read_frame = read_frame.values.reshape(sensor_sizes_dict[sensor][1])\n    linear_corr = linear_corr_frame.values.astype(np.float64).reshape(\n        [6] + sensor_sizes_dict[sensor][1]\n    )[:, cl:cr, :]\n\n    return (\n        signal,\n        dark_frame,\n        dead_frame,\n        linear_corr,\n        flat_frame,\n        # read_frame,\n    )\n\n\ndef ADC_convert(signal, gain, offset):\n    \"\"\"\n    Step 1: Analog-to-Digital Conversion (ADC) correction\n\n    The Analog-to-Digital Conversion (adc) is performed by the detector to convert the\n    pixel voltage into an integer number. We revert this operation by using the gain\n    and offset for the calibration files 'train_adc_info.csv'.\n    \"\"\"\n\n    return signal / gain + offset\n\n\ndef mask_hot_dead(signal, dead, dark):\n    \"\"\"\n    Step 2: Mask hot/dead pixel\n\n    The dead pixels map is a map of the pixels that do not respond to light and, thus,\n    can't be accounted for any calculation. In all these frames the dead pixels are\n    masked using python masked arrays. The bad pixels are thus masked but left\n    uncorrected. Some methods can be used to correct bad-pixels but this task,\n    if needed, is left to the participants.\n    \"\"\"\n\n    hot = sigma_clip(dark, sigma=5, maxiters=5).mask\n    hot = np.tile(hot, (signal.shape[0], 1, 1))\n    dead = np.tile(dead, (signal.shape[0], 1, 1))\n\n    # Set values to np.nan where dead or hot pixels are found\n    signal[dead] = np.nan\n    signal[hot] = np.nan\n    return signal\n\n\ndef apply_linear_corr(c, signal):\n    \"\"\"\n    Step 3: linearity Correction\n\n    The non-linearity of the pixels' response can be explained as capacitive leakage\n    on the readout electronics of each pixel during the integration time. The number\n    of electrons in the well is proportional to the number of photons that hit the\n    pixel, with a quantum efficiency coefficient. However, the response of the pixel\n    is not linear with the number of electrons in the well. This effect can be\n    described by a polynomial function of the number of electrons actually in the well.\n    The data is provided with calibration files linear_corr.parquet that are the\n    coefficients of the inverse polynomial function and can be used to correct this\n    non-linearity effect.\n    Using horner's method to evaluate the polynomial\n    \"\"\"\n    assert c.shape[0] == 6  # Ensure the polynomial is of degree 5\n\n    return (\n        (((c[5] * signal + c[4]) * signal + c[3]) * signal + c[2]) * signal + c[1]\n    ) * signal + c[0]\n\n\ndef clean_dark(signal, dark, dt):\n    \"\"\"\n    Step 4: dark current subtraction\n\n    The data provided include calibration for dark current estimation, which can be\n    used to pre-process the observations. Dark current represents a constant signal\n    that accumulates in each pixel during the integration time, independent of the\n    incoming light. To obtain the corrected image, the following conventional approach\n    is applied: The data provided include calibration files such as dark frames or\n    dead pixels' maps. They can be used to pre-process the observations. The dark frame\n    is a map of the detector response to a very short exposure time, to correct for the\n    dark current of the detector.\n\n    image - (dark * dt)\n\n    The corrected image is conventionally obtained via the following: where the dark\n    current map is first corrected for the dead pixel.\n    \"\"\"\n\n    dark = torch.tile(dark, (signal.shape[0], 1, 1))\n    signal -= dark * dt[:, None, None]\n    return signal\n\n\ndef get_cds(signal):\n    \"\"\"\n    Step 5: Get Correlated Double Sampling (CDS)\n\n    The science frames are alternating between the start of the exposure and the end of\n    the exposure. The lecture scheme is a ramp with a double sampling, called\n    Correlated Double Sampling (CDS), the detector is read twice, once at the start\n    of the exposure and once at the end of the exposure. The final CDS is the\n    difference (End of exposure) - (Start of exposure).\n    \"\"\"\n\n    return torch.subtract(signal[1::2, :, :], signal[::2, :, :])\n\n\ndef bin_obs(signal, binning):\n    \"\"\"\n    Step 5.1: Bin Observations\n\n    The data provided are binned in the time dimension. The binning is performed by\n    summing the signal over the time dimension.\n    \"\"\"\n\n    assert signal.shape[0] % binning == 0  # Ensure the binning is possible\n\n    # cds_transposed = signal.transpose(0, 2, 1)\n    cds_binned = torch.zeros(\n        (\n            signal.shape[0] // binning,\n            signal.shape[1],\n            signal.shape[2],\n        ),\n        device=\"cuda:0\",\n    )\n    for i in range(signal.shape[0] // binning):\n        cds_binned[i, :, :] = torch.sum(\n            signal[i * binning : (i + 1) * binning, :, :], axis=0\n        )\n    return cds_binned\n\n\ndef correct_flat_field(flat, signal):\n    \"\"\"\n    Step 6: Flat Field Correction\n\n    The flat field is a map of the detector response to uniform illumination, to\n    correct for the pixel-to-pixel variations of the detector, for example the\n    different quantum efficiencies of each pixel.\n    \"\"\"\n\n    return signal / flat\n\n\ndef nan_interpolation(tensor):\n    # Assume tensor is of shape (batch, height, width)\n    nan_mask = torch.isnan(tensor)\n\n    # Replace NaNs with zero temporarily\n    tensor_filled = torch.where(\n        nan_mask, torch.tensor(0.0, device=tensor.device), tensor\n    )\n\n    # Create a binary mask (0 where NaNs were and 1 elsewhere)\n    ones = torch.ones_like(tensor, device=tensor.device)\n    weight = torch.where(nan_mask, torch.tensor(0.0, device=tensor.device), ones)\n\n    # Perform interpolation by convolving with a kernel\n    # using bilinear interpolation\n    kernel = torch.ones(1, 1, 1, 3, device=tensor.device, dtype=tensor.dtype)\n\n    # Apply padding to the tensor and weight to prevent boundary issues\n    tensor_padded = F.pad(\n        tensor_filled.unsqueeze(1), (1, 1, 0, 0), mode=\"replicate\"\n    ).squeeze(1)\n    weight_padded = F.pad(weight.unsqueeze(1), (1, 1, 0, 0), mode=\"replicate\").squeeze(\n        1\n    )\n\n    # Convolve the filled tensor and the weight mask\n    tensor_conv = F.conv2d(tensor_padded.unsqueeze(1), kernel, stride=1)\n    weight_conv = F.conv2d(weight_padded.unsqueeze(1), kernel, stride=1)\n\n    # Compute interpolated values (normalized by weights)\n    interpolated_tensor = tensor_conv / weight_conv\n\n    # Apply the interpolated values only to the positions of NaNs\n    result = torch.where(nan_mask, interpolated_tensor.squeeze(1), tensor)\n\n    return result\n\n\ndef process_planet(planet_id):\n    \"\"\"\n    Process a single planet's data\n    \"\"\"\n    axis_info = pd.read_parquet(ROOT + \"axis_info.parquet\")\n    dt_airs = axis_info[\"AIRS-CH0-integration_time\"].dropna().values\n\n    for sensor in [\"FGS1\", \"AIRS-CH0\"]:\n        # load all data for this planet and sensor\n        signal, dark_frame, dead_frame, linear_corr, flat_frame = read_data(\n            planet_id, sensor, mode=MODE\n        )\n        gain, offset = get_gain_offset()\n\n        # Step 1: ADC correction\n        signal = ADC_convert(signal, gain, offset)\n\n        # Step 2: Mask hot/dead pixel\n        signal = mask_hot_dead(signal, dead_frame, dark_frame)\n\n        # clip at 0\n        signal = signal.clip(0)\n\n        # Step 3: linearity Correction\n        signal = apply_linear_corr(\n            torch.tensor(linear_corr).to(\"cuda:0\"), torch.tensor(signal).to(\"cuda:0\")\n        )\n\n        # Step 4: dark current subtraction\n        if sensor == \"FGS1\":\n            dt = torch.ones(len(signal), device=\"cuda:0\") * 0.1\n            dt[1::2] += 4.5\n        elif sensor == \"AIRS-CH0\":\n            dt = torch.tensor(dt_airs).to(\"cuda:0\")\n            dt[1::2] += 0.1\n\n        signal = clean_dark(signal, torch.tensor(dark_frame).to(\"cuda:0\"), dt)\n\n        # Step 5: Get Correlated Double Sampling (CDS)\n        signal = get_cds(signal)\n\n        # Step 5.1: Bin Observations\n        if sensor == \"FGS1\":\n            signal = bin_obs(signal, binning=BINNING * 12)\n        elif sensor == \"AIRS-CH0\":\n            signal = bin_obs(signal, binning=BINNING)\n\n        # Step 6: Flat Field Correction\n        signal = correct_flat_field(torch.tensor(flat_frame).to(\"cuda:0\"), signal)\n\n        # Step 7: Interpolate NaNs (twice!)\n        signal = nan_interpolation(signal)\n        signal = nan_interpolation(signal)\n\n        # Step 8: Sum over spatial axis\n        if sensor == \"FGS1\":\n            signal = torch.nanmean(signal, axis=[1, 2]).cpu().numpy()\n        elif sensor == \"AIRS-CH0\":\n            signal = torch.nanmean(signal, axis=1).cpu().numpy()\n\n        # save the processed signal\n        np.save(\n            f\"{planet_id}_{sensor}_signal_{VERSION}.npz\",\n            signal.astype(np.float64),\n        )\n\n\nif __name__ == \"__main__\":\n    star_info = pd.read_csv(ROOT + f\"/{MODE}_star_info.csv\")\n    star_info[\"planet_id\"] = star_info[\"planet_id\"].astype(int)\n    star_info = star_info.set_index(\"planet_id\")\n    planet_ids = star_info.index.tolist()\n\n    with mp.Pool(processes=4) as pool:\n        list(tqdm(pool.imap(process_planet, planet_ids), total=len(planet_ids)))\n\n    signal_train = []\n\n    for planet_id in planet_ids:\n        f_raw = np.load(f\"{planet_id}_FGS1_signal_{VERSION}.npz.npy\")\n        a_raw = np.load(f\"{planet_id}_AIRS-CH0_signal_{VERSION}.npz.npy\")\n\n        # flip a_raw\n        signal = np.concatenate([f_raw[:, None], a_raw[:, ::-1]], axis=1)\n        signal_train.append(signal)\n\n    signal_train = np.array(signal_train)\n    np.save(f\"signal_{VERSION}.npy\", signal_train, allow_pickle=False)\n\n    print(\"Processing complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-07-10T01:08:03.614052Z","iopub.execute_input":"2025-07-10T01:08:03.614548Z","iopub.status.idle":"2025-07-10T01:08:03.635808Z","shell.execute_reply.started":"2025-07-10T01:08:03.614528Z","shell.execute_reply":"2025-07-10T01:08:03.634966Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Writing preprocess.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python preprocess.py","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:03.636818Z","iopub.execute_input":"2025-07-10T01:08:03.637142Z","iopub.status.idle":"2025-07-10T01:08:21.572802Z","shell.execute_reply.started":"2025-07-10T01:08:03.637110Z","shell.execute_reply":"2025-07-10T01:08:21.571858Z"},"trusted":true},"outputs":[{"name":"stdout","text":"100%|█████████████████████████████████████████████| 1/1 [00:07<00:00,  7.59s/it]\nProcessing complete!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!rm -rf *FGS1_signal*","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:21.574399Z","iopub.execute_input":"2025-07-10T01:08:21.574841Z","iopub.status.idle":"2025-07-10T01:08:22.625900Z","shell.execute_reply.started":"2025-07-10T01:08:21.574805Z","shell.execute_reply":"2025-07-10T01:08:22.624808Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!rm -rf *AIRS-CH0_signal*","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:22.627356Z","iopub.execute_input":"2025-07-10T01:08:22.627730Z","iopub.status.idle":"2025-07-10T01:08:23.677338Z","shell.execute_reply.started":"2025-07-10T01:08:22.627700Z","shell.execute_reply":"2025-07-10T01:08:23.676132Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:23.681680Z","iopub.execute_input":"2025-07-10T01:08:23.682013Z","iopub.status.idle":"2025-07-10T01:08:24.740778Z","shell.execute_reply.started":"2025-07-10T01:08:23.681987Z","shell.execute_reply":"2025-07-10T01:08:24.739880Z"},"trusted":true},"outputs":[{"name":"stdout","text":"preprocess.py  signal_v2.npy\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\n\nfrom sklearn.linear_model import Ridge\nfrom scipy.signal import savgol_filter\nfrom scipy.optimize import curve_fit\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LinearRegression\n\n\nMODEL_VERSION = \"v1\"\nDATA_VERSION = \"v2\"\nPRE_BINNED_TIME = 15\n\nROOT = \"/kaggle/input/ariel-data-challenge-2025/\"\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    MODE = \"test\"\nelse:\n    MODE = \"train\"\n\nstar_info = pd.read_csv(ROOT + f\"/{MODE}_star_info.csv\")\nstar_info[\"planet_id\"] = star_info[\"planet_id\"].astype(int)\nstar_info = star_info.set_index(\"planet_id\")\nwavelengths = pd.read_csv(ROOT + \"/wavelengths.csv\")\n\nsignal_train = np.load(f\"signal_{DATA_VERSION}.npy\")\ncut_inf, cut_sup = 36, 318\nsignal_train = np.concatenate(\n    [signal_train[:, :, 0][:, :, None], signal_train[:, :, cut_inf:cut_sup]], axis=2\n)\nprint(signal_train.shape)\n# signal_train = signal_train.mean(axis=2)\n\ndef smooth_data(data, window_size):\n    return savgol_filter(data, window_size, 3)  # window size 51, polynomial order 3\n\n\n# find transit zones\ndef phase_detector(signal_orig, binning=15, smooth_window=11, verbose=False):\n    signal = signal_orig.reshape(-1, binning).mean(-1)  # collapse by 15; 375\n    signal = savgol_filter(signal, smooth_window, 2)  # smooth\n    first_derivative = np.gradient(signal)\n    phase1 = np.argmin(first_derivative)\n    phase2 = np.argmax(first_derivative)\n\n    if verbose:\n        plt.plot(signal_orig, color=\"grey\", alpha=0.5, label=\"original\")\n        plt.plot(signal, color=\"blue\", alpha=0.9, label=\"smoothed\")\n        plt.axvline(phase1, color=\"r\")\n        plt.axvline(phase2, color=\"r\")\n        plt.show()\n        plt.plot(first_derivative, color=\"green\", alpha=0.9, label=\"first derivative\")\n        plt.show()\n\n    assert phase1 < phase2\n    assert phase1 >= 0\n    assert phase2 <= signal.shape[0]\n    return phase1 * binning, phase2 * binning\n\n\ndef get_breakpoints(x, pre_binned_time, verbose=False):\n    bp = np.zeros(x.shape[0], dtype=np.int32)\n    bp2 = np.zeros(x.shape[0], dtype=np.int32)\n    for i in range(x.shape[0]):\n        signal = x[i].mean(-1)\n        p1, p2 = phase_detector(\n            signal, binning=15 // pre_binned_time, smooth_window=19, verbose=verbose\n        )\n        bp[i] = p1\n        bp2[i] = p2\n\n    return [bp, bp2]\n\n\n# breakpoint detection\nall_bp, all_bp2 = get_breakpoints(signal_train, PRE_BINNED_TIME, verbose=False)\n\n\ndef poly_exp_fit(data, optimized_breakpoints, buffer_size, degree=3):\n    # Define the three regions\n    x1 = np.arange(optimized_breakpoints[0] - buffer_size)\n    y1 = data[: optimized_breakpoints[0] - buffer_size]\n\n    x2 = np.arange(\n        optimized_breakpoints[0] + buffer_size,\n        optimized_breakpoints[1] - buffer_size,\n    )\n    y2 = data[\n        optimized_breakpoints[0] + buffer_size : optimized_breakpoints[1] - buffer_size\n    ]\n\n    x3 = np.arange(optimized_breakpoints[1] + buffer_size, len(data))\n    y3 = data[optimized_breakpoints[1] + buffer_size :]\n\n    # Concatenate the x-values and y-values for regions 1 and 3\n    x_combined = np.concatenate([x1, x3])\n    y_combined = np.concatenate([y1, y3])\n\n    def fit_function(x, *params):\n        poly_params = params[: degree + 1]\n        y_fit = np.polyval(poly_params, x)\n        return y_fit\n\n    # Define the polynomial fit function with an additional shift parameter for region 2\n    def fit_function_with_shift(x, shift, *poly_params):\n        x1_adjusted = x[: len(x1)]\n        x2_adjusted = x[len(x1) : len(x1) + len(x2)]\n        x3_adjusted = x[len(x1) + len(x2) :]\n        y1_fit = np.polyval(poly_params, x1_adjusted)\n        y2_fit = np.polyval(poly_params, x2_adjusted) * shift\n        y3_fit = np.polyval(poly_params, x3_adjusted)\n        return np.concatenate([y1_fit, y2_fit, y3_fit])\n\n    # Define the combined x-values (including region 2)\n    x_combined_with_region2 = np.concatenate([x1, x2, x3])\n    y_combined_with_region2 = np.concatenate([y1, y2, y3])\n\n    # Initial guesses for the polynomial coefficients and shift\n    poly_guess = np.polyfit(x_combined, y_combined, degree)\n\n    p0 = list(poly_guess)\n\n    initial_shift_guess = 1.0\n    p0 = [initial_shift_guess] + list(p0)\n\n    # Fit the polynomial and the shift using curve_fit\n    popt, _ = curve_fit(\n        fit_function_with_shift,\n        x_combined_with_region2,\n        y_combined_with_region2,\n        p0=p0,\n        maxfev=10000,\n    )\n\n    # Extract the optimized shift and polynomial coefficients\n    optimized_shift = popt[0]\n    assert optimized_shift > 0.8\n    optimized_poly_params = popt[1:]\n\n    return fit_function, optimized_poly_params, optimized_shift\n\n\ndef feature_engineering(signal_train):\n    \"\"\"Create a dataframe with two features from the raw data.\n\n    Parameters:\n    f_raw: ndarray of shape (n_planets, 67500)\n    a_raw: ndarray of shape (n_planets, 5625)\n\n    Return value:\n    df: DataFrame of shape (n_planets, 2)\n    \"\"\"\n\n    y_shifts = []\n\n    for IDX in tqdm(range(len(signal_train))):\n        data = signal_train[IDX]\n\n        buffer_size_poly = 150 // PRE_BINNED_TIME\n\n        optimized_breakpoints = [all_bp[IDX].item(), all_bp2[IDX].item()]\n\n        fit_func, params, y_shift = poly_exp_fit(\n            data[:, 1:].mean(1) / data[:, 1:].mean(1).mean(),\n            optimized_breakpoints,\n            buffer_size_poly,\n            degree=2,\n        )\n\n        y_shifts.append(y_shift)\n\n    y_shifts = np.array(y_shifts)\n\n    df = pd.DataFrame(\n        1 - y_shifts,\n        index=star_info.index,\n    )\n\n    return df\n\n\ndf = feature_engineering(signal_train)","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:24.742152Z","iopub.execute_input":"2025-07-10T01:08:24.742432Z","iopub.status.idle":"2025-07-10T01:08:26.258011Z","shell.execute_reply.started":"2025-07-10T01:08:24.742407Z","shell.execute_reply":"2025-07-10T01:08:26.257116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(1, 375, 283)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 69.09it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.259024Z","iopub.execute_input":"2025-07-10T01:08:26.259275Z","iopub.status.idle":"2025-07-10T01:08:26.272544Z","shell.execute_reply.started":"2025-07-10T01:08:26.259255Z","shell.execute_reply":"2025-07-10T01:08:26.271740Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                  0\nplanet_id          \n1103775    0.016606","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>planet_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1103775</th>\n      <td>0.016606</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"predictions = df.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T01:08:26.273541Z","iopub.execute_input":"2025-07-10T01:08:26.273882Z","iopub.status.idle":"2025-07-10T01:08:26.281125Z","shell.execute_reply.started":"2025-07-10T01:08:26.273854Z","shell.execute_reply":"2025-07-10T01:08:26.280312Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"predictions.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T01:08:26.282172Z","iopub.execute_input":"2025-07-10T01:08:26.282415Z","iopub.status.idle":"2025-07-10T01:08:26.294955Z","shell.execute_reply.started":"2025-07-10T01:08:26.282395Z","shell.execute_reply":"2025-07-10T01:08:26.294041Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(1, 1)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"star_info = pd.read_csv(ROOT + f\"/{MODE}_star_info.csv\")\nstar_info[\"planet_id\"] = star_info[\"planet_id\"].astype(int)\nstar_info = star_info.set_index(\"planet_id\")\nstar_info","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.296004Z","iopub.execute_input":"2025-07-10T01:08:26.296226Z","iopub.status.idle":"2025-07-10T01:08:26.318823Z","shell.execute_reply.started":"2025-07-10T01:08:26.296208Z","shell.execute_reply":"2025-07-10T01:08:26.318012Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                 Rs      Ms          Ts        Mp    e         P       sma  \\\nplanet_id                                                                    \n1103775    0.965432  0.9591  5539.03037  1.665007  0.0  6.932871  15.43293   \n\n                   i  \nplanet_id             \n1103775    89.533139  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rs</th>\n      <th>Ms</th>\n      <th>Ts</th>\n      <th>Mp</th>\n      <th>e</th>\n      <th>P</th>\n      <th>sma</th>\n      <th>i</th>\n    </tr>\n    <tr>\n      <th>planet_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1103775</th>\n      <td>0.965432</td>\n      <td>0.9591</td>\n      <td>5539.03037</td>\n      <td>1.665007</td>\n      <td>0.0</td>\n      <td>6.932871</td>\n      <td>15.43293</td>\n      <td>89.533139</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def postprocessing(pred_array, index, sigma_pred):\n    \"\"\"Create a submission dataframe from its components\n\n    Parameters:\n    pred_array: ndarray of shape (n_samples, 283)\n    index: pandas.Index of length n_samples with name 'planet_id'\n    sigma_pred: series of length n_samples or float\n\n    Return value:\n    df: DataFrame of shape (n_samples, 566) with planet_id as index\n    \"\"\"\n    if isinstance(sigma_pred, float):\n        expanded_sigmas = np.ones(len(pred_array)) * sigma_pred\n    else:\n        expanded_sigmas = sigma_pred\n\n    expanded_sigmas = np.repeat(expanded_sigmas[:, np.newaxis], 283, axis=1)\n    if pred_array.shape[1] == 1:\n        pred_array = np.repeat(pred_array, 283, axis=1)\n    return pd.concat(\n        [\n            pd.DataFrame(\n                pred_array.clip(0, None), index=index, columns=wavelengths.columns\n            ),\n            pd.DataFrame(\n                expanded_sigmas,\n                index=index,\n                columns=[f\"sigma_{i}\" for i in range(1, 284)],\n            ),\n        ],\n        axis=1,\n    )","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.319774Z","iopub.execute_input":"2025-07-10T01:08:26.320059Z","iopub.status.idle":"2025-07-10T01:08:26.326248Z","shell.execute_reply.started":"2025-07-10T01:08:26.320039Z","shell.execute_reply":"2025-07-10T01:08:26.325390Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"sub_df = postprocessing(predictions, star_info.index, sigma_pred=0.0008)","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.327150Z","iopub.execute_input":"2025-07-10T01:08:26.327352Z","iopub.status.idle":"2025-07-10T01:08:26.347368Z","shell.execute_reply.started":"2025-07-10T01:08:26.327336Z","shell.execute_reply":"2025-07-10T01:08:26.346684Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"sub_df","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.348629Z","iopub.execute_input":"2025-07-10T01:08:26.349024Z","iopub.status.idle":"2025-07-10T01:08:26.378912Z","shell.execute_reply.started":"2025-07-10T01:08:26.348993Z","shell.execute_reply":"2025-07-10T01:08:26.377828Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"               wl_1      wl_2      wl_3      wl_4      wl_5      wl_6  \\\nplanet_id                                                               \n1103775    0.016606  0.016606  0.016606  0.016606  0.016606  0.016606   \n\n               wl_7      wl_8      wl_9     wl_10  ...  sigma_274  sigma_275  \\\nplanet_id                                          ...                         \n1103775    0.016606  0.016606  0.016606  0.016606  ...     0.0008     0.0008   \n\n           sigma_276  sigma_277  sigma_278  sigma_279  sigma_280  sigma_281  \\\nplanet_id                                                                     \n1103775       0.0008     0.0008     0.0008     0.0008     0.0008     0.0008   \n\n           sigma_282  sigma_283  \nplanet_id                        \n1103775       0.0008     0.0008  \n\n[1 rows x 566 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wl_1</th>\n      <th>wl_2</th>\n      <th>wl_3</th>\n      <th>wl_4</th>\n      <th>wl_5</th>\n      <th>wl_6</th>\n      <th>wl_7</th>\n      <th>wl_8</th>\n      <th>wl_9</th>\n      <th>wl_10</th>\n      <th>...</th>\n      <th>sigma_274</th>\n      <th>sigma_275</th>\n      <th>sigma_276</th>\n      <th>sigma_277</th>\n      <th>sigma_278</th>\n      <th>sigma_279</th>\n      <th>sigma_280</th>\n      <th>sigma_281</th>\n      <th>sigma_282</th>\n      <th>sigma_283</th>\n    </tr>\n    <tr>\n      <th>planet_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1103775</th>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>...</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 566 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"sub_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.380153Z","iopub.execute_input":"2025-07-10T01:08:26.380469Z","iopub.status.idle":"2025-07-10T01:08:26.395957Z","shell.execute_reply.started":"2025-07-10T01:08:26.380446Z","shell.execute_reply":"2025-07-10T01:08:26.395178Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.396907Z","iopub.execute_input":"2025-07-10T01:08:26.397134Z","iopub.status.idle":"2025-07-10T01:08:26.429204Z","shell.execute_reply.started":"2025-07-10T01:08:26.397116Z","shell.execute_reply":"2025-07-10T01:08:26.428276Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   planet_id      wl_1      wl_2      wl_3      wl_4      wl_5      wl_6  \\\n0    1103775  0.016606  0.016606  0.016606  0.016606  0.016606  0.016606   \n\n       wl_7      wl_8      wl_9  ...  sigma_274  sigma_275  sigma_276  \\\n0  0.016606  0.016606  0.016606  ...     0.0008     0.0008     0.0008   \n\n   sigma_277  sigma_278  sigma_279  sigma_280  sigma_281  sigma_282  sigma_283  \n0     0.0008     0.0008     0.0008     0.0008     0.0008     0.0008     0.0008  \n\n[1 rows x 567 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>planet_id</th>\n      <th>wl_1</th>\n      <th>wl_2</th>\n      <th>wl_3</th>\n      <th>wl_4</th>\n      <th>wl_5</th>\n      <th>wl_6</th>\n      <th>wl_7</th>\n      <th>wl_8</th>\n      <th>wl_9</th>\n      <th>...</th>\n      <th>sigma_274</th>\n      <th>sigma_275</th>\n      <th>sigma_276</th>\n      <th>sigma_277</th>\n      <th>sigma_278</th>\n      <th>sigma_279</th>\n      <th>sigma_280</th>\n      <th>sigma_281</th>\n      <th>sigma_282</th>\n      <th>sigma_283</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1103775</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>0.016606</td>\n      <td>...</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n      <td>0.0008</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 567 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"!rm signal_v0.npy preprocess.py","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:26.430145Z","iopub.execute_input":"2025-07-10T01:08:26.430375Z","iopub.status.idle":"2025-07-10T01:08:27.486665Z","shell.execute_reply.started":"2025-07-10T01:08:26.430356Z","shell.execute_reply":"2025-07-10T01:08:27.485803Z"},"trusted":true},"outputs":[{"name":"stdout","text":"rm: cannot remove 'signal_v0.npy': No such file or directory\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:27.488097Z","iopub.execute_input":"2025-07-10T01:08:27.488375Z","iopub.status.idle":"2025-07-10T01:08:28.552849Z","shell.execute_reply.started":"2025-07-10T01:08:27.488350Z","shell.execute_reply":"2025-07-10T01:08:28.551970Z"},"trusted":true},"outputs":[{"name":"stdout","text":"signal_v2.npy  submission.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Adapted from https://www.kaggle.com/code/metric/ariel-gaussian-log-likelihood\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef competition_score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    naive_mean: float,\n    naive_sigma: float,\n    sigma_true: float = 0.00001,\n    row_id_column_name: str = \"planet_id\",\n) -> float:\n    \"\"\"\n    This is a Gaussian Log Likelihood based metric. For a submission, which contains\n    the predicted mean (x_hat) and variance (x_hat_std), we calculate the Gaussian\n    Log-likelihood (GLL) value to the provided ground truth(x). We treat each pair\n    of x_hat, x_hat_std as a 1D gaussian, meaning there will be 283 1D gaussian\n    distributions, hence 283 values for each test spectrum, the GLL value for one\n    spectrum is the sum of all of them.\n\n    Inputs:\n        - solution: Ground Truth spectra (from test set)\n            - shape: (nsamples, n_wavelengths)\n        - submission: Predicted spectra and errors (from participants)\n            - shape: (nsamples, n_wavelengths*2)\n        naive_mean: (float) mean from the train set.\n        naive_sigma: (float) standard deviation from the train set.\n        sigma_true: (float) essentially sets the scale of the outputs.\n    \"\"\"\n\n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n\n    if submission.min().min() < 0:\n        raise ParticipantVisibleError(\"Negative values in the submission\")\n    for col in submission.columns:\n        if not pd.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f\"Submission column {col} must be a number\")\n\n    n_wavelengths = len(solution.columns)\n    if len(submission.columns) != n_wavelengths * 2:\n        raise ParticipantVisibleError(\"Wrong number of columns in the submission\")\n\n    y_pred = submission.iloc[:, :n_wavelengths].values\n    # Set a non-zero minimum sigma pred to prevent division by zero errors.\n    sigma_pred = np.clip(\n        submission.iloc[:, n_wavelengths:].values, a_min=10**-15, a_max=None\n    )\n    y_true = solution.values\n\n    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred))\n    GLL_true = np.sum(\n        scipy.stats.norm.logpdf(\n            y_true, loc=y_true, scale=sigma_true * np.ones_like(y_true)\n        )\n    )\n    GLL_mean = np.sum(\n        scipy.stats.norm.logpdf(\n            y_true,\n            loc=naive_mean * np.ones_like(y_true),\n            scale=naive_sigma * np.ones_like(y_true),\n        )\n    )\n\n    submit_score = (GLL_pred - GLL_mean) / (GLL_true - GLL_mean)\n    return float(np.clip(submit_score, 0.0, 1.0))\n","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:28.554540Z","iopub.execute_input":"2025-07-10T01:08:28.554963Z","iopub.status.idle":"2025-07-10T01:08:28.565270Z","shell.execute_reply.started":"2025-07-10T01:08:28.554927Z","shell.execute_reply":"2025-07-10T01:08:28.564547Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    train_labels = pd.read_csv(ROOT + \"/train.csv\", index_col=\"planet_id\")\n    \n    gll_score = competition_score(\n        train_labels.copy().reset_index(),\n        sub_df.copy().reset_index(),\n        naive_mean=train_labels.values.mean(),\n        naive_sigma=train_labels.values.std(),\n    )\n\n    print(f\"# Estimated competition score: {gll_score:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2025-07-10T01:08:28.566351Z","iopub.execute_input":"2025-07-10T01:08:28.566967Z","iopub.status.idle":"2025-07-10T01:08:28.583630Z","shell.execute_reply.started":"2025-07-10T01:08:28.566938Z","shell.execute_reply":"2025-07-10T01:08:28.583016Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Reslut:\n\nScore: 0.328\n\nRank: 24 (2025-0710-11:29, JST)\n\nYour Best Entry!\nYour most recent submission scored 0.328, which is an improvement of your previous score of 0.292. Great job!\n\nRank 24. A small price to pay for my social life. #kaggle - https://kaggle.com/competitions/ariel-data-challenge-2025 ","metadata":{}}]}